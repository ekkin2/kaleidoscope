{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Ranking Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from snorkel.labeling import labeling_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The Pandas DataFrame will consist of the following fields: \n",
    "* **Source**\n",
    "* **Title** \n",
    "* **Content**\n",
    "* **Bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mappings for convenience\n",
    "ABSTAIN = -1\n",
    "LEFT = 0\n",
    "CENTER = 1\n",
    "RIGHT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_source_eval(x): \n",
    "    if x.bias == 'hyper_left':\n",
    "        return LEFT \n",
    "    \n",
    "@labeling_function()\n",
    "def lf_contains_left_keywords(x): \n",
    "    # todo: if the article contains a substantial amount\n",
    "    # of these phrases, then classify as left, else, center\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_right_keywords(x): \n",
    "    # todo: if the article contains a substantial amount\n",
    "    # of these phrases, then classify as left\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model\n",
    "\n",
    "This deep learning model accepts as input a tokenized corpus of text (news article) and learns to output a polarity ranking on a scale from 0 to 1.\n",
    "\n",
    "The steps for the deep learning model are outlined below: \n",
    "* Have pandas df with text, and label\n",
    "* Preprocess raw article text and tokenize. \n",
    "* Embedding it using Word2Vec method \n",
    "* Use gensim word encoder and average all the vectors from the article.\n",
    "* Train a logistic regression model with all features\n",
    "\n",
    "Initial Task: Binary Classifcation (Left vs. Right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "* May want to experiment with: https://github.com/UKPLab/sentence-transformers for sentence embedding instead of word by word embedding\n",
    "* Also could try Big Bird Transformer if we have time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(): \n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    content = data['content']\n",
    "    labels = data['bias']\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output text and labels \n",
    "def load_wv():\n",
    "    # this line of code needs to run asynchronously first\n",
    "    wv = api.load('word2vec-google-news-300')\n",
    "    return wv\n",
    "\n",
    "def clean_text(x): \n",
    "    '''\n",
    "    This function takes in raw text as a string, and cleans it\n",
    "    by removing extraneous text. \n",
    "    '''\n",
    "#     nltk.download()\n",
    "#     stop_words = stopwords.words(\"english\")\n",
    "    x = x.lower()\n",
    "#     x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x\n",
    "\n",
    "def preprocess_text(text): \n",
    "    X = []\n",
    "    found_words = []\n",
    "\n",
    "    text = clean_text(text)\n",
    "    words = text.split()\n",
    "    for word in words: \n",
    "        try:\n",
    "            # creates 300 vectors per word\n",
    "            found_words.append(wv[word])\n",
    "#             print('here')\n",
    "        except: \n",
    "            continue\n",
    "      \n",
    "    embedding = np.asarray(found_words)\n",
    "    mean = np.mean(embedding, axis=0)\n",
    "    mean = mean.tolist()\n",
    "\n",
    "    if type(mean) == list: \n",
    "        X.append(mean)\n",
    "\n",
    "    X = np.array(X)\n",
    "    return X\n",
    "\n",
    "def preprocess_labels(labels): \n",
    "    '''\n",
    "    This function converts text labels to numerical labels \n",
    "    '''\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo: for gautham, load this on init\n",
    "# wv = load_wv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "print(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y):\n",
    "    X_dset = []\n",
    "    y_dset = []\n",
    "    # for each transcript and label\n",
    "    for text, lbl in tqdm(zip(X, y)):\n",
    "        X_processed = preprocess_text(text)\n",
    "        X_processed = X_processed.tolist()\n",
    "        \n",
    "        if type(mean) == list: \n",
    "            X_dset.append(X_processed)\n",
    "            y_dset.append(lbl)\n",
    "    \n",
    "    X_dset = np.asarray(X_dset)\n",
    "    y_dset = np.array(y_dset)\n",
    "    return X_dset, y_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0ec3fc356583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_dset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "X_dset, y_dset = create_dataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_dset shape: ', X_dset.shape)\n",
    "print('y_dset shape: ', y_dset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model \n",
    "* Train model in Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X_dset, y_dset, val_size=0.15, test_size=0.15): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_dset, y_dset, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    print('X_train shape: ', X_train.shape)\n",
    "    print('y_train shape: ', y_train.shape)\n",
    "    \n",
    "    print('X_test shape: ', X_test.shape)\n",
    "    print('y_test shape: ', y_test.shape)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_dsets(X_train, X_test, y_train, y_test, batch_size=16): \n",
    "    train_dset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    test_dset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "    return train_dset, test_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(X_dset, y_dset)\n",
    "train_dset, test_dset = create_dsets(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 128\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_units): \n",
    "    model = tf.keras.Sequential()\n",
    "    input_layer = tf.keras.layers.Input(shape=(300,))\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "    # hidden_layer_3 = tf.keras.layers.Dense(hidden_units/2, activation='relu')\n",
    "    output_layer = tf.keras.layers.Dense(3, activation='softmax')\n",
    "    model.add(input_layer)\n",
    "    model.add(hidden_layer_1)\n",
    "    model.add(hidden_layer_2)\n",
    "    # model.add(hidden_layer_3)\n",
    "    model.add(output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                  metrics=['binary_accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = create_model(HIDDEN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(p1, p2=None, title='Plot', x_label='', y_label='', p1_legend=None, p2_legend=None): \n",
    "    plt.figure()\n",
    "    plt.plot(range(len(p1)), p1, label=p1_legend)\n",
    "    if p2 is not None: \n",
    "        plt.plot(range(len(p2)), p2, label=p2_legend)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_dset, validation_data=test_dset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hist.history['loss'], p2=hist.history['val_loss'], \n",
    "             title='Training and Validation Loss', x_label='Epoch', y_label='Loss Value', \n",
    "            p1_legend='Training loss', p2_legend='Validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = './polarity_model'\n",
    "model.save(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ai_impression(transcript, wv):\n",
    "    embedding = preprocess_text()\n",
    "\n",
    "    model_filepath = './static/models/ted_analysis_model'\n",
    "    model = tf.keras.models.load_model(model_filepath)\n",
    "    \n",
    "    pred = model.predict(embedding)[0]\n",
    "    cols = ['Beautiful', 'Confusing', 'Courageous', 'Funny', 'Informative', 'Ingenious', 'Inspiring', 'Longwinded', 'Unconvincing', 'Fascinating', 'Jaw-dropping', 'Persuasive', 'OK', 'Obnoxious']\n",
    "\n",
    "    ted_dict = {}\n",
    "    for val, col in zip(pred, cols):\n",
    "        ted_dict[col] = val\n",
    "    \n",
    "    word_result = max(ted_dict, key=ted_dict.get)\n",
    "\n",
    "    return word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(text, model_filepath): \n",
    "    '''\n",
    "    This function takes in the raw text of an article, preprocesses it, and computes\n",
    "    a polarity score as a double from the deep learning model. \n",
    "    '''\n",
    "    \n",
    "    embedding = preprocess_text(text)\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_filepath)\n",
    "    pred = model.predict(embedding)[0]\n",
    "    \n",
    "    def compute_polarity(pred): \n",
    "        '''\n",
    "        This function computes a weighted sum of labels to get a quantitative\n",
    "        polarity score.\n",
    "        '''\n",
    "        left = pred[0]\n",
    "        center = pred[1]\n",
    "        right = pred[2]\n",
    "        \n",
    "        polarity = left*0 + center*(0.5) + right*(1)\n",
    "        return polarity\n",
    "    \n",
    "    polarity = compute_polarity(pred)\n",
    "    \n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.load_model(model_filepath)\n",
    "\n",
    "# polarity = get_polarity(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_alg():\n",
    "    wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
