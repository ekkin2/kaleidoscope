{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Ranking Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from snorkel.labeling import labeling_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The Pandas DataFrame will consist of the following fields: \n",
    "* **Source**\n",
    "* **Title** \n",
    "* **Content**\n",
    "* **Bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mappings for convenience\n",
    "ABSTAIN = -1\n",
    "LEFT = 0\n",
    "CENTER = 1\n",
    "RIGHT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = pd.read_csv('all_phrases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7973\n"
     ]
    }
   ],
   "source": [
    "print(len(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>total_occurrences</th>\n",
       "      <th>bias_score</th>\n",
       "      <th>p_dem</th>\n",
       "      <th>p_rep</th>\n",
       "      <th>n_dem</th>\n",
       "      <th>n_rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health</td>\n",
       "      <td>21070</td>\n",
       "      <td>-0.347822</td>\n",
       "      <td>2.731629</td>\n",
       "      <td>1.321768</td>\n",
       "      <td>15278</td>\n",
       "      <td>5562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>senator</td>\n",
       "      <td>17989</td>\n",
       "      <td>-0.084759</td>\n",
       "      <td>1.925979</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>10772</td>\n",
       "      <td>6838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>president</td>\n",
       "      <td>16663</td>\n",
       "      <td>-0.099725</td>\n",
       "      <td>1.835509</td>\n",
       "      <td>1.502614</td>\n",
       "      <td>10266</td>\n",
       "      <td>6323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>act</td>\n",
       "      <td>15494</td>\n",
       "      <td>-0.050733</td>\n",
       "      <td>1.630610</td>\n",
       "      <td>1.473146</td>\n",
       "      <td>9120</td>\n",
       "      <td>6199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>senate</td>\n",
       "      <td>13020</td>\n",
       "      <td>0.060186</td>\n",
       "      <td>1.248167</td>\n",
       "      <td>1.408032</td>\n",
       "      <td>6981</td>\n",
       "      <td>5925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phrase  total_occurrences  bias_score     p_dem     p_rep  n_dem  n_rep\n",
       "0     health              21070   -0.347822  2.731629  1.321768  15278   5562\n",
       "1    senator              17989   -0.084759  1.925979  1.625000  10772   6838\n",
       "2  president              16663   -0.099725  1.835509  1.502614  10266   6323\n",
       "3        act              15494   -0.050733  1.630610  1.473146   9120   6199\n",
       "4     senate              13020    0.060186  1.248167  1.408032   6981   5925"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_occurrences</th>\n",
       "      <th>bias_score</th>\n",
       "      <th>p_dem</th>\n",
       "      <th>p_rep</th>\n",
       "      <th>n_dem</th>\n",
       "      <th>n_rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7973.000000</td>\n",
       "      <td>7973.000000</td>\n",
       "      <td>7973.000000</td>\n",
       "      <td>7973.000000</td>\n",
       "      <td>7973.000000</td>\n",
       "      <td>7973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>396.731594</td>\n",
       "      <td>-0.118258</td>\n",
       "      <td>0.043922</td>\n",
       "      <td>0.035070</td>\n",
       "      <td>245.653205</td>\n",
       "      <td>147.574313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>958.806287</td>\n",
       "      <td>0.336175</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.083713</td>\n",
       "      <td>612.992919</td>\n",
       "      <td>352.264237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>-0.305778</td>\n",
       "      <td>0.009297</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>-0.116600</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>0.013070</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>329.000000</td>\n",
       "      <td>0.061310</td>\n",
       "      <td>0.036832</td>\n",
       "      <td>0.029705</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21070.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.731629</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>15278.000000</td>\n",
       "      <td>6838.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_occurrences   bias_score        p_dem        p_rep         n_dem  \\\n",
       "count        7973.000000  7973.000000  7973.000000  7973.000000   7973.000000   \n",
       "mean          396.731594    -0.118258     0.043922     0.035070    245.653205   \n",
       "std           958.806287     0.336175     0.109600     0.083713    612.992919   \n",
       "min            50.000000    -1.000000     0.000000     0.000000      0.000000   \n",
       "25%            87.000000    -0.305778     0.009297     0.007129     52.000000   \n",
       "50%           150.000000    -0.116600     0.016092     0.013070     90.000000   \n",
       "75%           329.000000     0.061310     0.036832     0.029705    206.000000   \n",
       "max         21070.000000     1.000000     2.731629     1.625000  15278.000000   \n",
       "\n",
       "             n_rep  \n",
       "count  7973.000000  \n",
       "mean    147.574313  \n",
       "std     352.264237  \n",
       "min       0.000000  \n",
       "25%      30.000000  \n",
       "50%      55.000000  \n",
       "75%     125.000000  \n",
       "max    6838.000000  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3478220371078708\n",
      "-0.0847594171885326\n",
      "-0.09972509372533812\n",
      "-0.050733143606857656\n",
      "0.060185598644335335\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for row in phrases.iterrows():\n",
    "    i += 1\n",
    "    if i > 5: \n",
    "        break\n",
    "    print(row[1]['bias_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x): \n",
    "    '''\n",
    "    This function takes in raw text as a string, and cleans it\n",
    "    by removing extraneous text. \n",
    "    '''\n",
    "#     nltk.download()\n",
    "#     stop_words = stopwords.words(\"english\")\n",
    "    x = x.lower()\n",
    "#     x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_source_eval(x): \n",
    "    if x.bias == 'left':\n",
    "        return LEFT \n",
    "    elif x.bias == 'center':\n",
    "        return CENTER\n",
    "    elif x.bias == 'right':\n",
    "        return RIGHT\n",
    "    \n",
    "def count_occurrences(word, content):\n",
    "    return content.count(word)\n",
    "\n",
    "# def thresholds(): \n",
    "#     return \n",
    "    \n",
    "@labeling_function()\n",
    "def lf_phrase_score(x): \n",
    "    '''\n",
    "    Given x, a row in a pandas dataframe, determine how many times an \n",
    "    article contains a set number of phrases\n",
    "    '''\n",
    "    # todo: if the article contains a substantial amount\n",
    "    # of these phrases, then classify as left, else, center\n",
    "    text = clean_text(x.content)\n",
    "    \n",
    "    # for each phrase\n",
    "    sum_score = 0\n",
    "    sum_count = 0\n",
    "    for row in phrases.iterrows():\n",
    "        phrase = row[1]['phrase']\n",
    "        count = count_occurences(phrase, text)\n",
    "        score = count * row[1]['bias_score']\n",
    "        sum_score = sum_score + score \n",
    "        sum_count = sum_count + count\n",
    "    avg = sum_score / sum_count\n",
    "    \n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_left_keywords(x): \n",
    "    # todo: if the article contains a substantial amount\n",
    "    # of these phrases, then classify as left\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_right_keywords(x): \n",
    "    # todo: if the article contains a substantial amount\n",
    "    # of these phrases, then classify as left\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model\n",
    "\n",
    "This deep learning model accepts as input a tokenized corpus of text (news article) and learns to output a polarity ranking on a scale from 0 to 1.\n",
    "\n",
    "The steps for the deep learning model are outlined below: \n",
    "* Have pandas df with text, and label\n",
    "* Preprocess raw article text and tokenize. \n",
    "* Embedding it using Word2Vec method \n",
    "* Use gensim word encoder and average all the vectors from the article.\n",
    "* Train a logistic regression model with all features\n",
    "\n",
    "Initial Task: Binary Classifcation (Left vs. Right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "* May want to experiment with: https://github.com/UKPLab/sentence-transformers for sentence embedding instead of word by word embedding\n",
    "* Also could try Big Bird Transformer if we have time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(): \n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    content = data['content']\n",
    "    labels = data['bias']\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output text and labels \n",
    "def load_wv():\n",
    "    # this line of code needs to run asynchronously first\n",
    "    wv = api.load('word2vec-google-news-300')\n",
    "    return wv\n",
    "\n",
    "def preprocess_text(text): \n",
    "    X = []\n",
    "    found_words = []\n",
    "\n",
    "    text = clean_text(text)\n",
    "    words = text.split()\n",
    "    for word in words: \n",
    "        try:\n",
    "            # creates 300 vectors per word\n",
    "            found_words.append(wv[word])\n",
    "#             print('here')\n",
    "        except: \n",
    "            continue\n",
    "      \n",
    "    embedding = np.asarray(found_words)\n",
    "    mean = np.mean(embedding, axis=0)\n",
    "    mean = mean.tolist()\n",
    "\n",
    "    if type(mean) == list: \n",
    "        X.append(mean)\n",
    "\n",
    "    X = np.array(X)\n",
    "    return X\n",
    "\n",
    "def preprocess_labels(labels): \n",
    "    '''\n",
    "    This function converts text labels to numerical labels \n",
    "    '''\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo: for gautham, load this on init\n",
    "# wv = load_wv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_txt = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(count_occurrences('lorem ipsum', clean_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "print(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y):\n",
    "    X_dset = []\n",
    "    y_dset = []\n",
    "    # for each transcript and label\n",
    "    for text, lbl in tqdm(zip(X, y)):\n",
    "        X_processed = preprocess_text(text)\n",
    "        X_processed = X_processed.tolist()\n",
    "        \n",
    "        if type(mean) == list: \n",
    "            X_dset.append(X_processed)\n",
    "            y_dset.append(lbl)\n",
    "    \n",
    "    X_dset = np.asarray(X_dset)\n",
    "    y_dset = np.array(y_dset)\n",
    "    return X_dset, y_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0ec3fc356583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_dset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "X_dset, y_dset = create_dataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_dset shape: ', X_dset.shape)\n",
    "print('y_dset shape: ', y_dset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model \n",
    "* Train model in Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X_dset, y_dset, val_size=0.15, test_size=0.15): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_dset, y_dset, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    print('X_train shape: ', X_train.shape)\n",
    "    print('y_train shape: ', y_train.shape)\n",
    "    \n",
    "    print('X_test shape: ', X_test.shape)\n",
    "    print('y_test shape: ', y_test.shape)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_dsets(X_train, X_test, y_train, y_test, batch_size=16): \n",
    "    train_dset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    test_dset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "    return train_dset, test_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(X_dset, y_dset)\n",
    "train_dset, test_dset = create_dsets(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 128\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_units): \n",
    "    model = tf.keras.Sequential()\n",
    "    input_layer = tf.keras.layers.Input(shape=(300,))\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "    # hidden_layer_3 = tf.keras.layers.Dense(hidden_units/2, activation='relu')\n",
    "    output_layer = tf.keras.layers.Dense(3, activation='softmax')\n",
    "    model.add(input_layer)\n",
    "    model.add(hidden_layer_1)\n",
    "    model.add(hidden_layer_2)\n",
    "    # model.add(hidden_layer_3)\n",
    "    model.add(output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                  metrics=['binary_accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = create_model(HIDDEN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(p1, p2=None, title='Plot', x_label='', y_label='', p1_legend=None, p2_legend=None): \n",
    "    plt.figure()\n",
    "    plt.plot(range(len(p1)), p1, label=p1_legend)\n",
    "    if p2 is not None: \n",
    "        plt.plot(range(len(p2)), p2, label=p2_legend)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_dset, validation_data=test_dset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hist.history['loss'], p2=hist.history['val_loss'], \n",
    "             title='Training and Validation Loss', x_label='Epoch', y_label='Loss Value', \n",
    "            p1_legend='Training loss', p2_legend='Validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = './polarity_model'\n",
    "model.save(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ai_impression(transcript, wv):\n",
    "    embedding = preprocess_text()\n",
    "\n",
    "    model_filepath = './static/models/ted_analysis_model'\n",
    "    model = tf.keras.models.load_model(model_filepath)\n",
    "    \n",
    "    pred = model.predict(embedding)[0]\n",
    "    cols = ['Beautiful', 'Confusing', 'Courageous', 'Funny', 'Informative', 'Ingenious', 'Inspiring', 'Longwinded', 'Unconvincing', 'Fascinating', 'Jaw-dropping', 'Persuasive', 'OK', 'Obnoxious']\n",
    "\n",
    "    ted_dict = {}\n",
    "    for val, col in zip(pred, cols):\n",
    "        ted_dict[col] = val\n",
    "    \n",
    "    word_result = max(ted_dict, key=ted_dict.get)\n",
    "\n",
    "    return word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(text, model_filepath): \n",
    "    '''\n",
    "    This function takes in the raw text of an article, preprocesses it, and computes\n",
    "    a polarity score as a double from the deep learning model. \n",
    "    '''\n",
    "    \n",
    "    embedding = preprocess_text(text)\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_filepath)\n",
    "    pred = model.predict(embedding)[0]\n",
    "    \n",
    "    def compute_polarity(pred): \n",
    "        '''\n",
    "        This function computes a weighted sum of labels to get a quantitative\n",
    "        polarity score.\n",
    "        '''\n",
    "        left = pred[0]\n",
    "        center = pred[1]\n",
    "        right = pred[2]\n",
    "        \n",
    "        polarity = left*0 + center*(0.5) + right*(1)\n",
    "        return polarity\n",
    "    \n",
    "    polarity = compute_polarity(pred)\n",
    "    \n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.load_model(model_filepath)\n",
    "\n",
    "# polarity = get_polarity(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_alg():\n",
    "    wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
