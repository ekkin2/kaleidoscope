{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Ranking Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from snorkel.labeling import labeling_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The Pandas DataFrame will consist of the following fields: \n",
    "* **Source**\n",
    "* **Title** \n",
    "* **Content**\n",
    "* **Bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Pandas dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mappings for convenience\n",
    "ABSTAIN = -1\n",
    "LEFT = 0\n",
    "CENTER = 1\n",
    "RIGHT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_source_eval(x): \n",
    "    if x.bias == 'hyper_left':\n",
    "        return LEFT \n",
    "    \n",
    "@labeling_function()\n",
    "def lf_contains_left_keywords(x): \n",
    "    # todo: if the article contains a substantial amount\n",
    "    # of these phrases, then classify as left, else, center\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_right_keywords(x): \n",
    "    # todo: if the article contains a substantial amount\n",
    "    # of these phrases, then classify as left\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model\n",
    "\n",
    "This deep learning model accepts as input a tokenized corpus of text (news article) and learns to output a polarity ranking on a scale from 0 to 1.\n",
    "\n",
    "The steps for the deep learning model are outlined below: \n",
    "* Have pandas df with text, and label\n",
    "* Preprocess raw article text and tokenize. \n",
    "* Embedding it using Word2Vec method \n",
    "* Use gensim word encoder and average all the vectors from the article.\n",
    "* Train a logistic regression model with all features\n",
    "\n",
    "Initial Task: Binary Classifcation (Left vs. Right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "* May want to experiment with: https://github.com/UKPLab/sentence-transformers for sentence embedding instead of word by word embedding\n",
    "* Also could try Big Bird Transformer if we have time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(): \n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    content = data['content']\n",
    "    labels = data['bias']\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output text and labels \n",
    "def load_wv():\n",
    "    # this line of code needs to run asynchronously first\n",
    "    wv = api.load('word2vec-google-news-300')\n",
    "    return wv\n",
    "\n",
    "def preprocess_text(text): \n",
    "    X = []\n",
    "    found_words = []\n",
    "\n",
    "    words = text.split()\n",
    "    for word in words: \n",
    "        try:\n",
    "            # creates 300 vectors per word\n",
    "            found_words.append(wv[word])\n",
    "#             print('here')\n",
    "        except: \n",
    "            continue\n",
    "      \n",
    "    embedding = np.asarray(found_words)\n",
    "    mean = np.mean(embedding, axis=0)\n",
    "    mean = mean.tolist()\n",
    "\n",
    "    if type(mean) == list: \n",
    "        X.append(mean)\n",
    "\n",
    "    X = np.array(X)\n",
    "    return X\n",
    "\n",
    "def preprocess_labels(labels): \n",
    "    '''\n",
    "    This function converts text labels to numerical labels \n",
    "    '''\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo: for gautham, load this on init\n",
    "wv = load_wv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y):\n",
    "    X_dset = []\n",
    "    y_dset = []\n",
    "    # for each transcript and label\n",
    "    for text, lbl in tqdm(zip(X, y)):\n",
    "        X_processed = preprocess_text(text)\n",
    "        X_processed = X_processed.tolist()\n",
    "        \n",
    "        if type(mean) == list: \n",
    "            X_dset.append(X_processed)\n",
    "            y_dset.append(lbl)\n",
    "    \n",
    "    X_dset = np.asarray(X_dset)\n",
    "    y_dset = np.array(y_dset)\n",
    "    return X_dset, y_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "X_dset, y_dset = create_dataset(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
